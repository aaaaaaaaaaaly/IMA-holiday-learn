```python
import numpy as np
import pandas as pd
import matplotlib as mpl
```

#### 先处理数据


```python
from sklearn.datasets import load_diabetes
# 从sklearn库数据集里面导入糖尿病数据集,糖尿病数据集叫load_diabetes()

#数据命名
diabetes=load_diabetes()
# 打印观察，发现有data，feature_names，target
#变量赋值,获取一维数据
data=diabetes['data']
fname=diabetes['feature_names']
target=diabetes['target']
df=pd.DataFrame(data,columns=fname)
```



#### 正规方程

**思路：**

根据公式求出theta。

首先把数据集要测试的数据拉下来，type转化为列表，注意x需要补充1；

接着初始化theta，根据公式求出theta

由求出的theta，y=theta*x+b,画线



**代码实现（成功）**


```python
plt.figure(figsize=(8,5))
plt.plot(df['bmi'],df.index,'g*')
plt.title('bmi')
#以下的操作都需要将他们转化为列表,tolist方法
x=df['bmi'].tolist()
y=df.index.tolist()
#转化为矩阵
#创建一个空列表
z=[]
# 报错：asmatrix() missing 1 required positional argument: 'data'，所以要在第一列补上1
# 报错：'float' object has no attribute 'insert'，所以要用两个列表合并的方法zip
for i in range(len(x)):
    z.append(1)

X=[list(i) for i in zip(z,x)]
X=np.mat(X)
Y=np.mat(y)
theta=[[0],[0]]
#初始化theta
theta=11#带入公式
```

**改进**

分功能段，**用函数实现**

```python
import numpy as np
import pandas as pd
import matplotlib as mpl
from sklearn.datasets import load_diabetes
# 从sklearn库数据集里面导入糖尿病数据集,糖尿病数据集叫load_diabetes()

#数据命名
diabetes=load_diabetes()
# 打印观察，发现有data，feature_names，target
#变量赋值,获取一维数据
data_x=diabetes['data']
data_y=diabetes['target']
z=[]
for i in range((len(data_x))):
    z.append(1)
data_x=[list(i) for i in zip(z,data_x)]
def regular_experssion(data_x,data_y):
    #首先对数据进行处理,传进来的data_x，m*n（从数据集抓下来的初态）
    theta=np.linalg.inv(data_x.T.dot(data_x)).dot((data_x).T.dot(data_y.T))
    #求方程
    loss=np.mean(np.sum((data_x.dot(theta)-data_y)**2))
    return theta,loss
data_x=diabetes.data
data_y=diabetes.target
theta,loss= regular_experssion (data_x,data_y)
print(f"Implement theta:{theta}")
#格式化字符串
print(f"Implement loss:{loss}")
```



**画图**

此处仍待提高

```python
theta0=theta[0]
theta1=theta[1]
#画回归方程,
x=np.linspace(df['bmi'].reshape(-1).min(),df['bmi'].reshape(-1).max(),100)
y=theta1+theta0*x
plt.plot(x,y,c='r')
```

**出现的被解决的报错：**

 报错：'float' object has no attribute 'insert'，所以要用两个列表合并的方法zip

```
for i in range(len(x)):
    z.append(1)
x=[list(i) for i in zip(z,x)]
```

**未解决报错，在把x转化为矩阵的时候出现**


    TypeError           Traceback (most recent call last)
    
    <ipython-input-92-5da7725e61ca> in <module>
         14 # 报错：'float' object has no attribute 'insert'，所以要用两个列表合并的方法zip
         15 x=[list(i) for i in zip(z,x)]
    ---> 16 x=np.mat()
         17 y=np.mat()
         18 theta=[[0],[0]]

**已解决，报错原因：x=np.mat（x）**

------

#### 梯度下降实现线性回归

```python
theta=np.zeros((len(data_x[0])+1),1)#theta初始化为（n+1）*1的矩阵,n是特征数
data_x=hstack(np.ones((len(data_x),1)),data_x)#data_x是m*（n+1）矩阵
interations=10000#迭代次数
alpha=0.15#学习率
#代价函数
def costfunction(theta,data_x,data_y):
    predictvalue=data_x.dot(theta)-data_y#(m*1)
    return (predictvalue**2)/(2*len(data_x)),predictvalue#(m*1)
#求和放最后     
#梯度下降函数
def descendgradient(theta,data_x,data_y):
    costfunction(theta,data_x,data_y)
    costvector=[]#收集代价函数的值
    theta_history=[]#收集theta的变化
    #data_x的行数
    m=len(data_x)
    
    for i in range(0,iterations):
        temptheta=theta#(n+1)*1
        #每一次迭代根据上一次迭代的theta重新更新代价函数,记录
        cost,pred=costfunction(theta,data_x,data_y)
        costvector.append(cost)
        theta_history.append(list(theta[:,0]))
        gradient=np.zeros(((len(data_x[0])+1),1))#初始化梯度
        
        for j in range(0,len(theta)):#对于theta来说，行数就是特征数
            #j代表特征数，对于data_x来说就是列数
            #X代表每一列的集合
            #这里的pred用 列还未更新 的pred=theta*data_x-data_y,因为要同时更新
            X=np.array(x[:,j]).reshape(1,m)
            #或者直接X=data_x[:,j].T,变成（1*m）
            gradient[j,0]=np.sum(pred.dot(X))
        temptheta=temptheta-(alpha/m)*gradient
        theta=temptheta
        
     return theta,theta_history,costvector
```

#### 梯度下降实现逻辑回归

知识理解：

假设函数改变，代价函数随之改变，且变成非凸函数。

代价函数优化为凸函数，求最小：
$$
j（theta）=(-1/m)\sum\limits_{i=1}^m[ylog(h(x))+(1-y)log(1-h(x))]
$$
代码实现：

代价函数表达式改变，按照以上公式返回代价函数值

更新theta时，也无法像线性回归中只需要计算代价函数就可以获得pred值，从而直接简化求`hx`
$$
theta：=theta-(alpha/m)\sum\limits_{i=1}^m[(hx-y)*x_j^{(i)}]
$$
这个更新theta的函数是一样的；

但是代价函数的`hx`发生改变
$$
hx=1/(1+e^{-theta^{T}*x})
$$




