```python
import numpy as np
import pandas as pd
import matplotlib as mpl
```

#### 先处理数据


```python
from sklearn.datasets import load_diabetes
# 从sklearn库数据集里面导入糖尿病数据集,糖尿病数据集叫load_diabetes()

#数据命名
diabetes=load_diabetes()
# 打印观察，发现有data，feature_names，target
#变量赋值,获取一维数据
data=diabetes['data']
fname=diabetes['feature_names']
target=diabetes['target']
df=pd.DataFrame(data,columns=fname)
```



#### 正规方程

**思路：**

根据公式求出theta。

首先把数据集要测试的数据拉下来，type转化为列表，注意x需要补充1；

接着初始化theta，根据公式求出theta

由求出的theta，y=theta*x+b,画线



**代码实现（成功）**


```python
plt.figure(figsize=(8,5))
plt.plot(df['bmi'],df.index,'g*')
plt.title('bmi')
#以下的操作都需要将他们转化为列表,tolist方法
x=df['bmi'].tolist()
y=df.index.tolist()
#转化为矩阵
#创建一个空列表
z=[]
# 报错：asmatrix() missing 1 required positional argument: 'data'，所以要在第一列补上1
# 报错：'float' object has no attribute 'insert'，所以要用两个列表合并的方法zip
for i in range(len(x)):
    z.append(1)

X=[list(i) for i in zip(z,x)]
X=np.mat(X)
Y=np.mat(y)
theta=[[0],[0]]
#初始化theta
theta=11#带入公式
```

**改进**

分功能段，**用函数实现**

```python
import numpy as np
import pandas as pd
import matplotlib as mpl
from sklearn.datasets import load_diabetes
# 从sklearn库数据集里面导入糖尿病数据集,糖尿病数据集叫load_diabetes()

#数据命名
diabetes=load_diabetes()
# 打印观察，发现有data，feature_names，target
#变量赋值,获取一维数据
data_x=diabetes['data']
data_y=diabetes['target']
z=[]
for i in range((len(data_x))):
    z.append(1)
data_x=[list(i) for i in zip(z,data_x)]
def regular_experssion(data_x,data_y):
    #首先对数据进行处理,传进来的data_x，m*n（从数据集抓下来的初态）
    theta=np.linalg.inv(data_x.T.dot(data_x)).dot((data_x).T.dot(data_y.T))
    #求方程
    loss=np.mean(np.sum((data_x.dot(theta)-data_y)**2))
    return theta,loss
data_x=diabetes.data
data_y=diabetes.target
theta,loss= regular_experssion (data_x,data_y)
print(f"Implement theta:{theta}")
#格式化字符串
print(f"Implement loss:{loss}")
```



**画图**

此处仍待提高

```python
theta0=theta[0]
theta1=theta[1]
#画回归方程,
x=np.linspace(df['bmi'].reshape(-1).min(),df['bmi'].reshape(-1).max(),100)
y=theta1+theta0*x
plt.plot(x,y,c='r')
```

**出现的被解决的报错：**

 报错：'float' object has no attribute 'insert'，所以要用两个列表合并的方法zip

```
for i in range(len(x)):
    z.append(1)
x=[list(i) for i in zip(z,x)]
```

**未解决报错，在把x转化为矩阵的时候出现**


    TypeError           Traceback (most recent call last)
    
    <ipython-input-92-5da7725e61ca> in <module>
         14 # 报错：'float' object has no attribute 'insert'，所以要用两个列表合并的方法zip
         15 x=[list(i) for i in zip(z,x)]
    ---> 16 x=np.mat()
         17 y=np.mat()
         18 theta=[[0],[0]]

**已解决，报错原因：x=np.mat（x）**

------

#### 梯度下降实现线性回归

```python
theta=np.zeros((len(data_x[0])+1),1)#theta初始化为（n+1）*1的矩阵,n是特征数
data_x=hstack(np.ones((len(data_x),1)),data_x)#data_x是m*（n+1）矩阵
interations=10000#迭代次数
alpha=0.15#学习率
#代价函数
def costfunction(theta,data_x,data_y):
    predictvalue=data_x.dot(theta)-data_y#(m*1)
    return (predictvalue**2)/(2*len(data_x)),predictvalue#(m*1)  
#梯度下降函数
def descendgradient(theta,data_x,data_y):
    costfunction(theta,data_x,data_y)
    costvector=[]#收集代价函数的值
    theta_history=[]#收集theta的变化
    #data_x的行数
    m=len(data_x)
    
    for i in range(0,iterations):
        #每一次迭代根据上一次迭代的theta重新更新代价函数,记录
        cost,pred=costfunction(theta,data_x,data_y)
        costvector.append(cost)
        theta_history.append(list(theta[:,0]))
        gradient=np.zeros(((len(data_x[0])+1),1))#初始化梯度
        #对于theta来说，行数就是特征数
        #j代表特征数，对于data_x来说就是列数
        #X代表每一列的集合
        #这里的pred用 列还未更新 的pred=theta*data_x-data_y,因为要同时更新
        X=data_x.T,变成（1*m）
        gradient=np.sum(np.dot(pred，data_x.T))
        theta=theta-(alpha/m)*gradient

     return theta,theta_history,costvector
```

#### 梯度下降实现逻辑回归

知识理解：

假设函数改变，代价函数随之改变，且变成非凸函数。

代价函数优化为凸函数，求最小：
$$
j（theta）=(-1/m)\sum\limits_{i=1}^m[ylog(h(x))+(1-y)log(1-h(x))]
$$
代码实现：

代价函数表达式改变，按照以上公式返回代价函数值

更新theta时，也无法像线性回归中只需要计算代价函数就可以获得pred值，从而直接简化求`hx`
$$
theta：=theta-(alpha/m)\sum\limits_{i=1}^m[(hx-y)*x_j^{(i)}]
$$
这个更新theta的函数是一样的；

但是代价函数的`hx`发生改变
$$
hx=1/(1+e^{-theta^{T}*x})
$$





#### 决策树

![](https://s4.ax1x.com/2022/01/24/7T765q.jpg)



简单决策树的简单思路：

**目标是**：每一次选出现有特征中的信息增益最大的做节点，然后递归此函数知道没有特征数

如上图。

需要在这个求**现有特征中的信息增益最大**的函数**外**需要有一个函数，实现对传入的**dataset**，求:
$$
H(D)=\sum\limits^n_{i=1}\frac{|C_k|}{|D|}log_2\frac{|C_k|}{|D|}
$$
其实求某特征A的某个取值（比如A代表年龄，取值共有青年，中年，老年3种）：
$$
\sum\limits^k_{i=1}\frac{|C_{ik}|}{|D_{i}|}log_2\frac{|C_{ik}|}{|D_{k}|}
$$
这两个是**共同**的，只需要把特征A的**某个取值**从原列中**切出来**，只剩这个取值。那么就相当于，D_i，D都代表列表的样本总数，C_k,C_{`ik`}都表示求每种label取值的个数

```python
from math import log
#现在来定义这个外部函数，因为主要算经验熵，所以叫calent
def calEnt (dataset):
    #思路：只需要求出label的标签种类，以及每一个种类的个数
    #先对标签的种类进行筛选，放在字典中，这个字典叫labelcount
    labelcount={}
    #对label标签进行筛选
    for labels in dataset:
        currentlabel=labels[-1]#这个labels是dataset的每一行，-1找到最后一列
        if currentlabel not in labelcount.keys():
            labelcount[currentlabel]=0#把label的取值当做key，就是类似数组的下标
        labelcount[currentlabel]+=1
    #计算熵
    Ent=0
    for key in labelcount:
        prob=labelcount[key]/len(dataset)
        Ent+=prob*log(prob,2)
    return -Ent
```



回到这个求**现有特征中的信息增益最大**的函数，现在我们只需要实现一次，继续递归就可以达到种一棵树的目标

- 因为涉及求每一种特征的增益，所以需要有一个列表来存放每一次被切出来的**那个特征和末尾的label**

- 切完之后**进入**对这个特征求增益的**for循环**，因为该特征可能有不同取值，根据：

$$
H(D|A)=\sum\limits^n_{i=1}\frac{|D_i|}{|D|}\sum\limits^k_{i=1}\frac{|C_{ik}|}{|D_{i}|}log_2\frac{|C_{ik}|}{|D_{k}|}
$$

​     我们知道需要进行每种取值的求和，所以是for循环然后用  **+=**

​      因为已经切出来，所以D易知，D_i就需要我们继续把每种取值从特征A中切出来，也只剩下     这个取值和label，放在dataset里面，调用刚才说的**外**部函数,循环**+=**即可，求出所有的特征中增益最大的做节点**

- 继续调用本函数**（递归）**，参数是剩下的特征数，样本总数

```python
def splitfeature (featList, value):
    splitresult=[]
    for vote in featlist:
        if vote[-1]==value:
            splitresult.append(vote)
    return splitresult
def chooseBestFeatureToSplit(dataSet):
    #特征数量
    numFeatures = len(dataSet[0]) - 1
    #计数数据集的经验熵
    baseEntropy = calcShannonEnt(dataSet)
    #信息增益
    bestInfoGain = 0.0
    #最优特征的索引值
    bestFeature = -1
    #遍历所有特征
    for i in range(numFeatures):
        # 获取dataSet的第i个所有特征
        featList = [example[i] for example in dataSet]
        #example是dataset的每一行，因为是特征数i，列数，所以会把每一行的那一列放在featlist中然后i++
        Label=[example[-1]for example in dataSet]
        featList=featList.append(Label)
        #创建set集合{}，元素不可重复
        uniqueVals={}
        uniqueVals = set(featList)
        newEntropy=0
        for value in  uniqueVals:
            #subDataSet划分后的子集
            subdataset = splitDataSet(featList, value)
            #计算子集的概率
            prob = len(subdataset) / len(dataSet)
            #根据公式计算经验条件熵
            newEntropy += prob * calEnt((subdataset))
        #信息增益
        infoGain = baseEntropy - newEntropy
        if (infoGain > bestInfoGain):
            #更新信息增益，找到最大的信息增益
            bestInfoGain = infoGain
            #记录信息增益最大的特征的索引值
            bestFeature = i
            #返回信息增益最大特征的索引值
    return bestFeature
```

- 接下来就是种树的函数，需要不断递归自身，也包括很多情况的判断，主框架是  **If-then**

```python
import operator
#统计classlist中出现次数最多的元素（类标签）
def majorityCnt (classlist):
    classcount={}
    #统计classlist中每个元素出现的次数
    for vote in classlist:
        classcount[vote]=0
        classcount[vote]+=1
        #根据字典的值降序排列
        sortedclasscount=sorted(classcount.items(),key=operator.itemgetter(1),reverse=True)
        return sortedclasscount[0][0]
def splitDataSet(dataSet,axis,value):
    #去掉axis特征
    #创建返回的数据集列表
    retDataSet=[]
    #遍历数据集
    for featVec in dataSet:
        if featVec[axis]==value:
            #去掉axis特征
            reduceFeatVec=featVec[:axis]
            #将符合条件的添加到返回的数据集
            reduceFeatVec.extend(featVec[axis+1:])
            retDataSet.append(reduceFeatVec)
    #返回划分后的数据集
    return retDataSet
def planttree (dataset,labels,featlabels):
    #取分类标签
    classlist=[example[-1] for example in dataset]
    #按照课本的算法逻辑
    #如果类别相同，停止继续划分
    if classlist.count(classlist[0])==len(classlist):
        return classlist[0]
    #遍历完所有特征时返回次数最多的类标签
    if len(dataset[0])==1:
        return majorityCnt(classlist)
    #选择最优特征
    bestfeature= chooseBestFeatureToSplit(dataset)
    #找到最优特征的标签
    bestfeaturelabel=labels[bestfeature]
    featlabels.append(bestfeaturelabel)
    #生成树
    Tree={bestfeaturelabel:{}}
    #删除已经选择的标签
    del(labels[bestfeature])
    #获得训练集中所有最优特征的属性值
    featurevalues=[example[bestfeature] for example in dataset]
    #删除重复的属性值
    uniquevalues=set(featurevalues)    
    for value in uniquevalues:
        Tree[bestfeaturelabel][value]=planttree (splitDataSet(dataset,bestfeature,value),labels,featlabels)
```

##### 决策树的预剪枝

**分为预剪枝和后剪枝，剪枝被称做泛化过程**

- 后剪枝比较容易理解：

首先有衡量是否剪枝的标准：损失函数
$$
C_α(T)=\sum\limits^{|T|}_{t=1}N_tH_t(T)+α{|T|}
$$

- T是叶子节点，H_t(T)是第t个叶子节点的熵

从叶子节点开始递归，记其父节点将所有子节点回缩后的子树为Tb（分类值取类别比例最大的特征值），未回缩的子树为Ta，如果C α ( T a ) ≥ C α ( T b ) 回缩后使得损失函数减小了，那么应该使这棵子树回缩，递归直到无法回缩为止。


**wait**





#### 朴素贝叶斯

目标：简单来说：按照已有数据集，学得联合概率分布，对给定输入x求出后验概率最大的输出。

具体：先验概率分布和条件概率分布

（这样比较容易理解贝叶斯公式）

![](https://s4.ax1x.com/2022/01/27/7vAp1s.png)

而我们的朴素贝叶斯算法就是要通过给定数据集先学得**P(X,Y)**,
$$
P(X,Y)=P(Y)P(X|Y)
$$
其中：**P(Y)**是先验概率，**P(X|Y)**是条件概率（后验概率）

- 先算验概率（公式）：

$$
P(Y=c_k)=\sum\limits^N_{i=1}\frac{I(y_i=c_k)}{N},k=1,2,...,K
$$

- 后验概率（公式）：

$$
P(X^{j}=a_{ij}|Y=c_k)=\frac{\sum\limits^N_{i=1}I(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum\limits^N_{i=1}I(y_i=c_k)}
$$

$$
j=1,2,...,n;l=1,2,...,S_j;k=1,2,...,K
$$

- 再根据给定的x=(x^(1),x^(2),,...,x^(n))^T,计算：

$$
P(Y=c_k)\prod^n_{j=1}P(X^{(j)}=x^{i}|Y=c_k),k=1,2,...K
$$

- 最后确定实例x的类,求出y概率最大的值：

$$
y=argmax_{c_k}P(Y=c_k)\prod^n_{j=1}P(X^{(j)}=x^{i}|Y=c_k)
$$

**例子**

![](https://s4.ax1x.com/2022/01/27/7vn8Et.png)

![](https://s4.ax1x.com/2022/01/27/7vnEAx.png)

根据后来要测定的x的已有取值来测定这种情况下，y最大可能是什么值；比较简单的概率模型



![](https://s4.ax1x.com/2022/01/28/7zSpCR.png)



**代码实现：**

```python
#对于数据集，需要把x，y拉取出来，x里面包含特征，y就是要预测的值
```

- **首先是根据已经有的数据集，求出先验概率和后验概率**

```python
def Bayes_Second(i,j,x_y_counts,y_value_count):
    p_x_y=dict()
    for k in x_y_counts.index:
        #按照公式
        #i对应x的特征，j对应y的取值
        p_x_y[(i,k,j)]=x_y_counts[k]/y_value_count
    return p_x_y
def Bayes_First(x,y):
    #需要知道y有多少取值，且需要知道每一种取值各自有多少
    y_values=y[y.columns[0]].unique()
    y_value_count=y[y.columns[0]].value_counts()
    #unique()是找出一共有多少取值的函数
    #value_counts()是找出每一种取值有多少个的函数
    p_y_ck=y_value_count/len(y_values)
    #然后求x每一种特征的每一种取值在类内的比例
    for i in x.columns:
        #需要知道每一种y的取值x的某个特征的取值们各占多少
        for j in y_values:
            x_y_counts=x[(y==j).values][i].values.count_count()
            p_x_y=Bayes_Second(i,j,x_y_counts,y_value_count)
    return y_values,p_y_ck,p_x_y
```



- **最后是预测出y最大可能的值**

```python
import numpy as np
def y_max_predict(X,y_values,p_y_ck,p_x_y):
    y_results=[]
    for i in y_values:
        p_y=p_y_ck[i]
        p_X_y=0
        for j in X.items():
            p_X_y*=p_x_y[tuple(list(j)+[i])]
        y_results.append(p_y*p_X_y)   
    return y_values[np.argmax(y_results)]
```

(新的收获的函数相关知识补在另一个`md`文件)







#### 神经网络

##### 代价函数

逻辑回归的代价函数：
$$
J(theta)=\frac{-1}{m}\begin{bmatrix} \sum\limits^m_{i=1}y^{i}logh_{theta}(x^{(i)})+(1-y^{(i)})log(1-h_{theta}(x^{(i)}))\end{bmatrix}
$$
神经网络的代价函数：

- 因为神经网络的样本结果是多个，则需要按照每一个结果类别往回找，让每一个的代价函数都最小

$$
h_{theta}(x)\in\R^K,(h_{theta}(x))_{i}=i^{th}output
$$

$$
J(theta)=\frac{-1}{m}\begin{bmatrix} \sum\limits^m_{i=1}\sum\limits ^K_{k=1}y^{i}_klog(h_{theta}(x^{(i)}))_k+(1-y^{(i)})log(1-(h_{theta}(x^{(i)}))_k)\end{bmatrix}
$$

对参数进行最优值更新，依据就是代价函数最小

注意，需要排除每一层的theta_0,每一层theta矩阵平方和。

##### 反向传播

**为了最小化代价函数，需要求偏导**

![](https://s4.ax1x.com/2022/02/07/HMggTf.png)

**提供反向传播算法：**

![](https://s4.ax1x.com/2022/02/07/HMg7mq.png)

###### 流程：

- **首先**计算最后一层的误差:𝛿_(4)=a_(4)-y

$$
𝛿^{(l)}_j表示第l层第j个单元的误差
$$

- 然后继续向左求出各层的误差，直到倒数第2层（第一层是输入变量，不存在误差）





###### 计算各层误差的公式：

- 第**四**层误差是激活单元的预测a与实际值y之间的误差

$$
𝛿_j^{(4)}=a_j^{(4)}-y_j
$$

​       其中:
$$
a_j^{(4)}=(h_{theta}^{(x)})_j
$$

- 第**三**层误差：

$$
𝛿(3) = (𝛩^{(3)})^𝑇𝛿^{(4)} ∗ 𝑔′(𝑧^{(3)})
$$

​       其中：

​       (𝜃(3))^𝑇𝛿^(4)是权重导致的**误差的和**； 𝑔′(𝑧^(3))是激活函数（sigmoid)的导数，且       sigmoid函数的导数有一个特点：
$$
𝑔′(𝑧^{(3)}) = 𝑎^{(3)}∗(1−𝑎^{(3)})
$$

- 倒数第2层:

$$
𝛿(2) = (𝛩^{(2)})^𝑇𝛿^{(3)} ∗ 𝑔′(𝑧^{(2)})
$$

- 有了误差，就可以开始计算代价函数的偏导：

![](https://s4.ax1x.com/2022/02/07/HMWMY8.png)

L：表示目前是第几层

j：表示层内的激活单元下标（向左，**下一层第j个输入变量**的下标）

i：下一层误差单元的下标（向左，受到 权重矩阵中第i行 影响的下一层误差的下标）

![](https://s4.ax1x.com/2022/02/08/H3u2DI.png)

需要计算dz1，就需要计算：
$$
g^{[1]'}(Z^{[1]})，
$$
那么：
$$
g^{[1]′}(z^{[1]})=1−(a^{[1]})^2
$$
所以我们可以通过

`1-np.power(a^{[1]},2)`代替





###### 吴永恩的伪代码实现

![](https://s4.ax1x.com/2022/02/07/HMfdHI.png)

接着进行代价函数的偏导数计算：

D表示梯度

![](https://s4.ax1x.com/2022/02/07/HMfy8S.png)

###### 展开参数

真正实现的时候注意矩阵，向量之间的转化



##### 梯度检测

对于逻辑回归，我们初始化参数θ为0，但是神经网络不能初始化相同的值，因为如果第一层参数相同，意味着第二层所有激活单元的值会完全相同



取θ点左右各一点（θ+ε）,（θ-ε），则点θ的导数（梯度）近似等于(J(Θ+ε)-J(θ-ε))/(2ε)。

![](https://s4.ax1x.com/2022/02/07/HM4Ymd.png)

因此，针对每个θ，导数的近似值：

![](https://s4.ax1x.com/2022/02/07/HM4Bp8.png)

**伪代码：**

将这个近似值和反向传播每一步得到的 j 的导数D进行比较，如果结果相近，那么代码正确

![](https://s4.ax1x.com/2022/02/07/HM4fhV.png)



**随机初始化参数：**

![](https://s4.ax1x.com/2022/02/07/HMhci6.png)

##### 综合

正常情况下，隐藏层单元越多越好，但是这样无疑也会让计算量增大。所以选择好适合的神经网络结构至关重要



训练神经网络：

1. 参数的随机初始化
2.  利用**正向传播**方法计算所有的 ℎ𝜃(𝑥)、
3.  编写计算代价函数 𝐽 的代码，在正向传播计算出hx的同时求出代价函数
4.  利用**反向传播**方法计算所有偏导数（求出梯度D）
5.  利用**数值检验**（梯度检测）方法这些偏导
6. 使用优化算法来最小代价函数，求得最佳参数𝜃



![](https://s4.ax1x.com/2022/02/07/HM5KBj.png)

##### 代码实现：

- （把代码中比较难（^我的能力不足^）的反向传播po上）

- （这部分我是根据已经有的代码理解跟着打的，注意用1-np.power(A1,2)代替激活函数导数的方法

```
def backward_propagation(parameters,cache,X,Y):
     #反向传播算法，求出倒数第二层的误差就可以求偏导，求出最佳的theta
     m=X.shape[1]
     w1=parameters["w1"]
     w2=parameters["w2"]

     A1=cache["A1"]
     A2=cache["A2"]

     dz2=A2-Y
     dw2=(1/m)*np.dot(dz2,A1.T)
     db2=(1/m)*np.sum(dz2,axis=1,keepdims=True)
     dz1=np.multiply(np.dot(w2.T,dz2),1-np.power(A1,2))
     dw1=(1/m)*np.dot(dz1,X.T)
     db1=(1/m)*np.sum(dz1,axis=1,keepdims=True)
     grads={"dw1":dw1,
            "db1":db1,
            "dw2":dw2,
            "db2":db2}
     return grads
```



#### 降维

降维就是采用某种映射方法，将高位空间中的数据点映射到低位空间中

降维学习的本质就是学习一个映射函数：f:x->y

- x是原始数据点的表达，目前最多使用向量表达形式

- y是映射后的低位向量表达

降维有利于除去 **冗杂信息，噪音信息**，有利于提高识别以及精度



##### 主成分分析算法（PCA）理解

通过某种线性投影，将高维数据映射到低维的空间中表示，并且在**投影的维度上数据的方差最大**

通俗的理解，如果把所有的点都映射到一起，那么几乎所有的信息（如点和点之间的距离关系）都丢失了，而如果映射后方差尽可能的大，那么数据点则会 分散开来，以此来保留更多的信息。可以证明，PCA是丢失原始数据信息最少的一种线性降维方式。（实际上就是最接近原始数据，但是PCA并不试图去探索数据内在结构）



设 **n 维向量w**为目标子空间的一个坐标轴方向（称为映射向量），**最大化数据映射后的方差，有：**

![四大机器学习降维算法：PCA、LDA、LLE、Laplacian Eigenmaps](http://static.open-open.com/lib/uploadImg/20150330/20150330171353_726.gif)

-  m 是数据实例的个数

-  xi是数据实例 i 的向量表达

-  x拔是所有数据实例的平均向量。

- 定义W为**包含所有映射向量为列向量**的矩阵，经过线性代数变换，可以得到如下优化目标函数：

![四大机器学习降维算法：PCA、LDA、LLE、Laplacian Eigenmaps](http://static.open-open.com/lib/uploadImg/20150330/20150330171353_950.gif)

其中tr表示矩阵的迹， ![四大机器学习降维算法：PCA、LDA、LLE、Laplacian Eigenmaps](http://static.open-open.com/lib/uploadImg/20150330/20150330171353_841.gif) A是数据[协方差矩阵](https://so.csdn.net/so/search?q=协方差矩阵&spm=1001.2101.3001.7020)。

容易得到**最优的W**是由数据协方差矩阵**前 k 个最大的特征值**对应的特征向量作为列向量构成的。这些特征向量形成**一组正交基**并且最好地保留了数据中的信息。

PCA的输出就是Y = W‘X，由X的原始维度降**低到了k维**。



PCA追求的是在降维之后能够最大化保持数据的内在信息，并通过衡量在投影方向上的**数据方差的大小**来衡量**该方向**的重要性。



- 举例

具体可以看 下图所示，若使用PCA将数据点投影至一维空间上时，PCA会选择2轴，这使得原本很容易区分的两簇点被揉杂在一起变得无法区分；而这时若选择1轴将会得到很好的区分结果。

![四大机器学习降维算法：PCA、LDA、LLE、Laplacian Eigenmaps](http://static.open-open.com/lib/uploadImg/20150330/20150330171353_548.gif)

所以选择1轴方向更好





##### 从n维降到k维的步骤



使用PCA 从 n 维减少到 k 维:
1）**均值归一化**。

计算出所有特征的均值，然后令 xj = xj − μj 。如果特征是在不同的数量级上，还需要将其除以标准差 σ2 。

![img](https://img2018.cnblogs.com/blog/542362/201901/542362-20190103214200971-1053346551.png)





2）**计算协方差矩阵(covariance matrix) sigma Σ**

![img](https://img2018.cnblogs.com/blog/542362/201901/542362-20190103214340428-1732852297.png)

![img](https://img2018.cnblogs.com/blog/542362/201901/542362-20190103214647757-1759306087.png)





3）**计算协方差矩阵 Σ 的特征向量(eigenvectors):**
在 Matlab 里我们可以利用奇异值分解(singular value decomposition)来得到特则向量矩阵 U，调用方式为 [U， S，V] = svd(sigma) 。（注：函数返回的矩阵 S 也有用，后续会讲到）

![img](https://img2018.cnblogs.com/blog/542362/201901/542362-20190103214432659-115162024.png)

对于一个 n × n 维度的矩阵，U 是一个由 “与数据之间最小投射误差的方向向量” 构成的矩阵。 如果希望将数据从 n 维降至 k 维，只需要从 U 中选取前 k 个向量，获得一个 n × k 维度的矩阵，用Ureduce 表示，然后通过如下计算获得要求的新特征向量 z(i):

![img](https://img2018.cnblogs.com/blog/542362/201901/542362-20190103222013820-1010671799.png)

其中 Ureduce 为 n x k 维，x 为 n × 1 维，因此结果 z(i) 为 k × 1 维。 注：我们不对方差特征进行处理。

![img](https://img2018.cnblogs.com/blog/542362/201901/542362-20190103214506313-60932004.png)

###### 代码实现

- 有许多专业的函数方法，仔细学习！

```python
import numpy as np
"""
降维实现步骤：
均值归一化；
计算协方差矩阵
计算协方差矩阵的特征向量
"""
class PCA ():
    #计算协方差矩阵
    #均值归一化,本身减去均值，再除以方差
    def clac_cov(self,x):
        m=x.shape[0]
        x=(x-np.mean(x,axis=0))/np.var(x,axis=0)
       #矩阵相乘
        return 1/m * np.matmul(x.T,x)
    def pca(self,x,n_components):
        #计算协方差矩阵
        cov_matrix=self.calc_cov(x)
        #计算特征值，特征向量
        eigenvalues,eigenvectors=np.linalg.eig(cov_matrix)
        #对特征值排序
        idx=eigenvalues.argsort()[::-1]
        #取最大的前n_component组,变成k维
        eigenvectors=eigenvectors[:,idx]
        eigenvectors=eigenvectors[:,:n_components]

        #Y=pX转化
        return np.matmul(x,eigenvectors)
```



##### 局部线性嵌入（LLE）

Locally linear embedding（LLE）[1] 是一种非线性降维算法，它能够使降维后的数据较好地保持原有 **流形结构** 。LLE可以说是流形学习方法最经典的工作之一。很多后续的流形学习、降维方法都与LLE有密切联系。

见图1，使用LLE将三维数据（b）映射到二维（c）之后，映射后的数据仍能保持原有的数据流形（红色的点互相接近，蓝色的也互相接近），说明LLE有效地保持了数据原有的**流行结构**。

但是LLE在有些情况下也并不适用，如果数据分布在整个**封闭**的球面上，LLE则不能将它映射到二维空间，且不能保持原有的数据流形。那么我们在处理数据中，**首先假设数据不是分布在闭合的球面或者椭球面上。**

![四大机器学习降维算法：PCA、LDA、LLE、Laplacian Eigenmaps](http://static.open-open.com/lib/uploadImg/20150330/20150330171354_865.png)

图1 LLE降维算法使用实例



###### 理解

**部分公式还未深入理解，未推导，已经表面逻辑理解**

LLE算法认为每一个数据点都可以由其**近邻点的线性加权组合构造**得到。

算法的主要步骤分为三步：

（1）寻找每个样本点的k个近邻点

（2）由每个 样本点的近邻点计算出该样本点的**局部重建权值矩阵**

（3）由该样本点的局**部重建权值矩阵**和其**近邻点**计算出该样本点的输出值。具体的算法流程如图2所示：

![四大机器学习降维算法：PCA、LDA、LLE、Laplacian Eigenmaps](http://static.open-open.com/lib/uploadImg/20150330/20150330171354_97.png)

图 2 LLE算法步骤

![四大机器学习降维算法：PCA、LDA、LLE、Laplacian Eigenmaps](http://static.open-open.com/lib/uploadImg/20150330/20150330171354_474.jpg)

![四大机器学习降维算法：PCA、LDA、LLE、Laplacian Eigenmaps](http://static.open-open.com/lib/uploadImg/20150330/20150330171355_996.jpg)

#### k近邻



#### 聚类

把距离作为特征，通过 **自下而上**的迭代方式（即距离对比），快速把一群样本分类为几个类别

即把相似的对象归到同一个簇中，使**同一个簇内的对象的相似性尽可能大**，不在同一个簇中的数据对象的差异性也尽可能大

同一类的数据尽可能聚集在一起，不同的数据尽可能分离



- 类别

![](https://s4.ax1x.com/2022/02/11/Hao04x.png)



聚类是基于样本的 **相似度**，对数据进行归类

相似度的 **度量方法**：

1. 距离相似性度量
2. 密度相似性度量
3. 连通相似性度量



##### K-Means

1. 随机选取k个中心点
2. 计算**N个样本点**和**K个中心点**之间的 **欧氏距离**
3. 将每个样本点划分到最近的 （即欧氏距离最小的）**中心点类**，--------迭代1
4. 计算每个类别中**样本点的均值**，得到K个均值，将K个均值作为新的中心点-----迭代2
5. 重复2,3,4,步骤
6. 得到足够的收敛条件后，得到收敛后的K个中心点（中心点已经收敛）



K-Means聚类可以用欧式距离

欧式距离：

二维平面两个点的距离公式；

多维空间

比如样本类似于  a（x1，x2，...,xn），b（y1，y2，...yn)

公式：

![](https://s4.ax1x.com/2022/02/11/Hdk1gS.png)

**不仅可以欧式距离，还可以曼哈顿距离，马氏距离**

- 曼哈顿距离

![](https://s4.ax1x.com/2022/02/11/HdmfPA.png)

##### 代码实现K-Means

```python
# 计算欧拉距离
def calc_Dis(dataset, centroids, k):
    clalist=[]
    """
    对每个对象点进行分类，分类到距离最小的
    现在计算这个点和每一种类之间的距离
    """
    for data in dataset:
        diff=np.title(data,(k,1))-centroids
        squaredDiff=diff**2
        squaredDist=np.sum(squaredDiff,axis=1)#求行上的和
        distance=squaredDist**0.5
        clalist.append(distance)
    clalist=np.array(clalist)
    return clalist
def classify(dataset,centroids,k):
    """
    现在将每个样本点分到最近的中心点，即找出质点
    """
    clalist=clac_Dis(dataset,centroids,k)
    #计算出新的质心
    minDistIndices=np.argmin(clalist,axis=1)#求出每一行最小值的下标
    newCentroids=pd.DataFrame(dataset).groupby(minDistIndices).mean()
    """
    dataframte对dataset进行分组
    groupby（min)按照min进行分类
    mean（对分类结果求均值
    """
    newCentroids=newCentroids.values
    #计算变化量
    changed=newCentroids-centroids

    return changed,newCentroids
#kmeans分类
def kmeans(dataset,k):
    #随机化质心
    centroids=random.sample(dataset,k)
    #更新质心
    cluster=[]
    clalist=calc_Dis=(dataset,centroids,k)#求欧拉距离
    minDistIndices=np.argmin(clalist,axis=1)
    for i in range(k):
        cluster.append([])
    for i ,j in enumerate(minDistIndices):
    #enumerate可以同时遍历索引以及元素
            cluster[j].append(dataset[i])
    return centroids,cluster 
```



##### DBSCAN

具有噪声的基于密度的聚类方法，基于密度的空间聚类方法。

将具有足够密度的区域划分为簇，并在具有噪声的空间数据中发现任意形状的簇，将簇定义为密度相连的点的最大集合



这里提一下聚类算法中最常用的评估方法——轮廓系数：

![](https://s4.ax1x.com/2022/02/11/Hdd4KS.png)



-  计算样本i到同簇其它样本到**平均距离ai**。

  ai越小，说明样本i越应该被聚类到该簇（将ai称为样本i到簇内不相似度）。- 

-  计算样本i到**其它某簇Cj的所有样本的平均距离bij**，

  称为样本i与簇Cj的不相似度。

  定义为样本i的**簇间不相似度**：bi=min(bi1,bi2,...,bik2)

- si接近1，则说明样本i聚类合理

- si接近-1，则说明样本i更应该分类到另外的簇

- 若si近似为0，则说明样本i在两个簇的边界上



###### 代码实现







