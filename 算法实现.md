```python
import numpy as np
import pandas as pd
import matplotlib as mpl
```

#### 先处理数据


```python
from sklearn.datasets import load_diabetes
# 从sklearn库数据集里面导入糖尿病数据集,糖尿病数据集叫load_diabetes()

#数据命名
diabetes=load_diabetes()
# 打印观察，发现有data，feature_names，target
#变量赋值,获取一维数据
data=diabetes['data']
fname=diabetes['feature_names']
target=diabetes['target']
df=pd.DataFrame(data,columns=fname)
```



#### 正规方程

**思路：**

根据公式求出theta。

首先把数据集要测试的数据拉下来，type转化为列表，注意x需要补充1；

接着初始化theta，根据公式求出theta

由求出的theta，y=theta*x+b,画线



**代码实现（成功）**


```python
plt.figure(figsize=(8,5))
plt.plot(df['bmi'],df.index,'g*')
plt.title('bmi')
#以下的操作都需要将他们转化为列表,tolist方法
x=df['bmi'].tolist()
y=df.index.tolist()
#转化为矩阵
#创建一个空列表
z=[]
# 报错：asmatrix() missing 1 required positional argument: 'data'，所以要在第一列补上1
# 报错：'float' object has no attribute 'insert'，所以要用两个列表合并的方法zip
for i in range(len(x)):
    z.append(1)

X=[list(i) for i in zip(z,x)]
X=np.mat(X)
Y=np.mat(y)
theta=[[0],[0]]
#初始化theta
theta=11#带入公式
```

**改进**

分功能段，**用函数实现**

```python
import numpy as np
import pandas as pd
import matplotlib as mpl
from sklearn.datasets import load_diabetes
# 从sklearn库数据集里面导入糖尿病数据集,糖尿病数据集叫load_diabetes()

#数据命名
diabetes=load_diabetes()
# 打印观察，发现有data，feature_names，target
#变量赋值,获取一维数据
data_x=diabetes['data']
data_y=diabetes['target']
z=[]
for i in range((len(data_x))):
    z.append(1)
data_x=[list(i) for i in zip(z,data_x)]
def regular_experssion(data_x,data_y):
    #首先对数据进行处理,传进来的data_x，m*n（从数据集抓下来的初态）
    theta=np.linalg.inv(data_x.T.dot(data_x)).dot((data_x).T.dot(data_y.T))
    #求方程
    loss=np.mean(np.sum((data_x.dot(theta)-data_y)**2))
    return theta,loss
data_x=diabetes.data
data_y=diabetes.target
theta,loss= regular_experssion (data_x,data_y)
print(f"Implement theta:{theta}")
#格式化字符串
print(f"Implement loss:{loss}")
```



**画图**

此处仍待提高

```python
theta0=theta[0]
theta1=theta[1]
#画回归方程,
x=np.linspace(df['bmi'].reshape(-1).min(),df['bmi'].reshape(-1).max(),100)
y=theta1+theta0*x
plt.plot(x,y,c='r')
```

**出现的被解决的报错：**

 报错：'float' object has no attribute 'insert'，所以要用两个列表合并的方法zip

```
for i in range(len(x)):
    z.append(1)
x=[list(i) for i in zip(z,x)]
```

**未解决报错，在把x转化为矩阵的时候出现**


    TypeError           Traceback (most recent call last)
    
    <ipython-input-92-5da7725e61ca> in <module>
         14 # 报错：'float' object has no attribute 'insert'，所以要用两个列表合并的方法zip
         15 x=[list(i) for i in zip(z,x)]
    ---> 16 x=np.mat()
         17 y=np.mat()
         18 theta=[[0],[0]]

**已解决，报错原因：x=np.mat（x）**

------

#### 梯度下降实现线性回归

```python
theta=np.zeros((len(data_x[0])+1),1)#theta初始化为（n+1）*1的矩阵,n是特征数
data_x=hstack(np.ones((len(data_x),1)),data_x)#data_x是m*（n+1）矩阵
interations=10000#迭代次数
alpha=0.15#学习率
#代价函数
def costfunction(theta,data_x,data_y):
    predictvalue=data_x.dot(theta)-data_y#(m*1)
    return (predictvalue**2)/(2*len(data_x)),predictvalue#(m*1)  
#梯度下降函数
def descendgradient(theta,data_x,data_y):
    costfunction(theta,data_x,data_y)
    costvector=[]#收集代价函数的值
    theta_history=[]#收集theta的变化
    #data_x的行数
    m=len(data_x)
    
    for i in range(0,iterations):
        #每一次迭代根据上一次迭代的theta重新更新代价函数,记录
        cost,pred=costfunction(theta,data_x,data_y)
        costvector.append(cost)
        theta_history.append(list(theta[:,0]))
        gradient=np.zeros(((len(data_x[0])+1),1))#初始化梯度
        #对于theta来说，行数就是特征数
        #j代表特征数，对于data_x来说就是列数
        #X代表每一列的集合
        #这里的pred用 列还未更新 的pred=theta*data_x-data_y,因为要同时更新
        X=data_x.T,变成（1*m）
        gradient=np.sum(np.dot(pred，data_x.T))
        theta=theta-(alpha/m)*gradient

     return theta,theta_history,costvector
```

#### 梯度下降实现逻辑回归

知识理解：

假设函数改变，代价函数随之改变，且变成非凸函数。

代价函数优化为凸函数，求最小：
$$
j（theta）=(-1/m)\sum\limits_{i=1}^m[ylog(h(x))+(1-y)log(1-h(x))]
$$
代码实现：

代价函数表达式改变，按照以上公式返回代价函数值

更新theta时，也无法像线性回归中只需要计算代价函数就可以获得pred值，从而直接简化求`hx`
$$
theta：=theta-(alpha/m)\sum\limits_{i=1}^m[(hx-y)*x_j^{(i)}]
$$
这个更新theta的函数是一样的；

但是代价函数的`hx`发生改变
$$
hx=1/(1+e^{-theta^{T}*x})
$$





#### 决策树

![](https://s4.ax1x.com/2022/01/24/7T765q.jpg)



简单决策树的简单思路：

**目标是**：每一次选出现有特征中的信息增益最大的做节点，然后递归此函数知道没有特征数

如上图。

需要在这个求**现有特征中的信息增益最大**的函数**外**需要有一个函数，实现对传入的**dataset**，求:
$$
H(D)=\sum\limits^n_{i=1}\frac{|C_k|}{|D|}log_2\frac{|C_k|}{|D|}
$$
其实求某特征A的某个取值（比如A代表年龄，取值共有青年，中年，老年3种）：
$$
\sum\limits^k_{i=1}\frac{|C_{ik}|}{|D_{i}|}log_2\frac{|C_{ik}|}{|D_{k}|}
$$
这两个是**共同**的，只需要把特征A的**某个取值**从原列中**切出来**，只剩这个取值。那么就相当于，D_i，D都代表列表的样本总数，C_k,C_{`ik`}都表示求每种label取值的个数

```python
from math import log
#现在来定义这个外部函数，因为主要算经验熵，所以叫calent
def calEnt (dataset):
    #思路：只需要求出label的标签种类，以及每一个种类的个数
    #先对标签的种类进行筛选，放在字典中，这个字典叫labelcount
    labelcount={}
    #对label标签进行筛选
    for labels in dataset:
        currentlabel=labels[-1]#这个labels是dataset的每一行，-1找到最后一列
        if currentlabel not in labelcount.keys():
            labelcount[currentlabel]=0#把label的取值当做key，就是类似数组的下标
        labelcount[currentlabel]+=1
    #计算熵
    Ent=0
    for key in labelcount:
        prob=labelcount[key]/len(dataset)
        Ent+=prob*log(prob,2)
    return -Ent
```



回到这个求**现有特征中的信息增益最大**的函数，现在我们只需要实现一次，继续递归就可以达到种一棵树的目标

- 因为涉及求每一种特征的增益，所以需要有一个列表来存放每一次被切出来的**那个特征和末尾的label**

- 切完之后**进入**对这个特征求增益的**for循环**，因为该特征可能有不同取值，根据：

$$
H(D|A)=\sum\limits^n_{i=1}\frac{|D_i|}{|D|}\sum\limits^k_{i=1}\frac{|C_{ik}|}{|D_{i}|}log_2\frac{|C_{ik}|}{|D_{k}|}
$$

​     我们知道需要进行每种取值的求和，所以是for循环然后用  **+=**

​      因为已经切出来，所以D易知，D_i就需要我们继续把每种取值从特征A中切出来，也只剩下     这个取值和label，放在dataset里面，调用刚才说的**外**部函数,循环**+=**即可，求出所有的特征中增益最大的做节点**

- 继续调用本函数**（递归）**，参数是剩下的特征数，样本总数

```python
def splitfeature (featList, value):
    splitresult=[]
    for vote in featlist:
        if vote[-1]==value:
            splitresult.append(vote)
    return splitresult
def chooseBestFeatureToSplit(dataSet):
    #特征数量
    numFeatures = len(dataSet[0]) - 1
    #计数数据集的经验熵
    baseEntropy = calcShannonEnt(dataSet)
    #信息增益
    bestInfoGain = 0.0
    #最优特征的索引值
    bestFeature = -1
    #遍历所有特征
    for i in range(numFeatures):
        # 获取dataSet的第i个所有特征
        featList = [example[i] for example in dataSet]
        #example是dataset的每一行，因为是特征数i，列数，所以会把每一行的那一列放在featlist中然后i++
        Label=[example[-1]for example in dataSet]
        featList=featList.append(Label)
        #创建set集合{}，元素不可重复
        uniqueVals={}
        uniqueVals = set(featList)
        newEntropy=0
        for value in  uniqueVals:
            #subDataSet划分后的子集
            subdataset = splitDataSet(featList, value)
            #计算子集的概率
            prob = len(subdataset) / len(dataSet)
            #根据公式计算经验条件熵
            newEntropy += prob * calEnt((subdataset))
        #信息增益
        infoGain = baseEntropy - newEntropy
        if (infoGain > bestInfoGain):
            #更新信息增益，找到最大的信息增益
            bestInfoGain = infoGain
            #记录信息增益最大的特征的索引值
            bestFeature = i
            #返回信息增益最大特征的索引值
    return bestFeature
```

- 接下来就是种树的函数，需要不断递归自身，也包括很多情况的判断，主框架是  **If-then**

